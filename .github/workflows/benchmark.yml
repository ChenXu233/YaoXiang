name: Benchmark

on:
  push:
  #   branches: [main]
  # pull_request:
  #   branches: [main]
  # workflow_dispatch:
  #   inputs:
  #     iterations:
  #       description: "Number of iterations"
  #       required: false
  #       default: "100"
  #     regression_threshold:
  #       description: "Regression threshold (e.g., 0.05 for 5%)"
  #       required: false
  #       default: "0.05"

permissions:
  contents: read
  actions: read

jobs:
  benchmarks:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: nightly
          components: llvm-tools-preview

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: ''

      - name: Install Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'

      - name: Install C++ Compiler
        run: sudo apt-get update && sudo apt-get install -y g++ gdb make

      - name: Cache Rust
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo
            target
          key: ${{ runner.os }}-bench-${{ hashFiles('**/Cargo.toml') }}
          restore-keys: |
            ${{ runner.os }}-bench-

      - name: Build Release
        run: cargo build --release

      - name: Run YaoXiang Benchmarks
        run: |
          iterations=${{ github.event.inputs.iterations || '100' }}
          echo "Running with $iterations iterations..."
          cargo bench --all-features -- --sample-size=$iterations --output-format=json 2>&1 | tee benchmark_raw.json

      - name: Save Criterion Results
        uses: actions/upload-artifact@v4
        with:
          name: criterion-results
          path: target/criterion/
          retention-days: 30

      - name: Run Language Comparison
        run: |
          python3 scripts/compare.py --output compare_results.json --iterations 50

      - name: Save Comparison Results
        uses: actions/upload-artifact@v4
        with:
          name: comparison-results
          path: compare_results.json
          retention-days: 30

      - name: Check for Regressions
        if: github.event_name == 'pull_request'
        run: |
          threshold=${{ github.event.inputs.regression_threshold || '0.05' }}
          echo "Checking for regressions (threshold: $threshold)..."
          python3 -c '
          import json
          import sys

          try:
              with open("benchmark_raw.json", "r") as f:
                  data = json.load(f)

              for result in data.get("results", []):
                  name = result.get("name", "unknown")
                  print(f"Benchmark: {name}")

              print("Regression check completed.")
          except Exception as e:
              print(f"Warning: Could not check regressions: {e}", file=sys.stderr)
          '

      - name: Generate Benchmark Report
        run: |
          python3 scripts/generate_report.py --output benchmark-report/index.html

      - name: Upload Benchmark Report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report
          path: benchmark-report/
          retention-days: 30

  benchmark-summary:
    name: Benchmark Summary
    needs: benchmarks
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
      - name: Download Comparison Results
        uses: actions/download-artifact@v4
        with:
          name: comparison-results
          path: .

      - name: Generate Summary Comment
        run: |
          echo "## ðŸš€ Benchmark Results" > benchmark_summary.md
          echo "" >> benchmark_summary.md
          echo "### Language Performance Comparison" >> benchmark_summary.md
          echo "" >> benchmark_summary.md
          echo "| Benchmark | YaoXiang | Python | Rust | C++ | Go |" >> benchmark_summary.md
          echo "|-----------|-----------|--------|------|-----|----|" >> benchmark_summary.md

          if [ -f "compare_results.json" ]; then
            python3 scripts/generate_summary.py
          fi

          cat benchmark_summary.md
